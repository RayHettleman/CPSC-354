\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,trees}
\usepackage[hidelinks]{hyperref}

\newcommand{\ARSnode}[1]{%
  \node[circle,draw,minimum size=7mm,inner sep=0pt] (#1) {$#1$};%
}
\newcommand{\ARSnodeat}[2]{%
  \node[circle,draw,minimum size=7mm,inner sep=0pt, #2] (#1) {$#1$};%
}
\tikzset{
  >={Stealth[length=3mm]},
  every loop/.style={looseness=7},
  ptree/.style={level distance=10mm, sibling distance=18mm},
  nt/.style   ={draw, rounded corners, inner sep=1.2pt, font=\footnotesize\ttfamily, fill=gray!10},
  tm/.style   ={font=\footnotesize\ttfamily}
}

\title{CPSC-354 Report}
\author{Ray Hettleman \\ Chapman University \\ \texttt{rhettleman@chapman.edu}}
\date{September 2, 2025}

\begin{document}
\maketitle

\begin{abstract}
This report collects my weekly notes, homework, and reflections for CPSC-354. 
\end{abstract}

\tableofcontents
\newpage

% =========================================================
\section{Introduction}
This section intentionally left blank for now.

% =========================================================
\section{Week by Week}

% ---------- Week 1 ----------
\subsection{Week 1}

\subsubsection{Notes and Exploration}
We studied the MIU system from Hofstadter’s \emph{Gödel, Escher, Bach}. The task was to decide whether the string \texttt{MIII} can be derived from \texttt{MI} using the system’s rules.

\subsubsection{Homework}
\textbf{Rules.}
\begin{enumerate}[label=\Roman*.]
  \item If a string ends with \texttt{I}, you may append \texttt{U}.
  \item From \texttt{Mx} you may infer \texttt{Mxx}.
  \item Replace any occurrence of \texttt{III} by \texttt{U}.
  \item Delete any occurrence of \texttt{UU}.
\end{enumerate}

\textbf{Reasoning.}  
We begin with \texttt{MI}.  
Rule I allows \texttt{MIU}.  
Rule II doubles the sequence after \texttt{M}: \texttt{MI} $\Rightarrow$ \texttt{MII}, then \texttt{MIIII}, etc.  
Rule III can only replace consecutive \texttt{III} with a \texttt{U}.  
But because doubling produces powers of two \texttt{I}'s (1, 2, 4, 8, ...), 
we never get exactly three \texttt{I}'s.  
Thus, no sequence of rules produces \texttt{MIII}.  

\textbf{Conclusion.}  
It is impossible to derive \texttt{MIII} from \texttt{MI}.  
The parity of the number of \texttt{I}'s (always even after the first doubling) prevents reaching 3.

\subsubsection{Questions}
\emph{What is a rule that could be implemented that, while still requiring many steps, makes the MU-Puzzle solvable?}

% ---------- Week 2 ----------
\subsection{Week 2}

\subsubsection{Notes and Exploration}
We explored Abstract Reduction Systems (ARS), focusing on termination, confluence, and unique normal forms (UNFs). 
Each ARS was represented as a graph, and we determined its key properties.

\subsubsection{Homework}
\paragraph{1.\; $A=\varnothing$}
No nodes or edges.  
Terminating: True. Confluent: True. UNFs: True.

\paragraph{2.\; $A=\{a\},\; R=\varnothing$}
\begin{tikzpicture}
  \ARSnode{a};
\end{tikzpicture}

Normal forms: $a$. Terminating: True. Confluent: True. UNFs: True.

\paragraph{3.\; $A=\{a\},\; R=\{(a,a)\}$}
\begin{tikzpicture}
  \ARSnode{a};
  \path (a) edge[loop above] (a);
\end{tikzpicture}

Infinite sequence $a\to a\to\cdots$.  
Terminating: False. Confluent: True. UNFs: False.

\paragraph{4.\; $A=\{a,b,c\},\; R=\{(a,b),(a,c)\}$}
\begin{tikzpicture}[node distance=20mm]
  \ARSnode{a};
  \ARSnodeat{b}{right=of a}
  \ARSnodeat{c}{below=of b}
  \draw (a) edge (b) (a) edge (c);
\end{tikzpicture}

$b$ and $c$ are normal forms, so from $a$ two distinct endpoints are reachable.  
Terminating: True. Confluent: False. UNFs: False.

\paragraph{5.\; $A=\{a,b\},\; R=\{(a,a),(a,b)\}$}
\begin{tikzpicture}[node distance=22mm]
  \ARSnode{a};
  \ARSnodeat{b}{right=of a}
  \draw (a) edge[loop above] (a) (a) edge (b);
\end{tikzpicture}

$b$ is normal; all paths from $a$ can reach $b$.  
Terminating: False. Confluent: True. UNFs: True.

\paragraph{6.\; $A=\{a,b,c\},\; R=\{(a,b),(b,b),(a,c)\}$}
\begin{tikzpicture}[node distance=22mm]
  \ARSnode{a};
  \ARSnodeat{b}{right=of a}
  \ARSnodeat{c}{below=of b}
  \draw (a) edge (b) (a) edge (c) (b) edge[loop above] (b);
\end{tikzpicture}

Non-terminating through $b\to b$; $c$ is normal but unreachable from $b$.  
Terminating: False. Confluent: False. UNFs: False.

\paragraph{7.\; $A=\{a,b,c\},\; R=\{(a,b),(b,b),(a,c),(c,c)\}$}
\begin{tikzpicture}[node distance=22mm]
  \ARSnode{a};
  \ARSnodeat{b}{right=of a}
  \ARSnodeat{c}{below=of b}
  \draw (a) edge (b) (a) edge (c) (b) edge[loop above] (b) (c) edge[loop right] (c);
\end{tikzpicture}

Both $b$ and $c$ loop indefinitely.  
Terminating: False. Confluent: False. UNFs: False.

\textbf{Summary Table:}
\begin{center}
\begin{tabular}{@{}clccc@{}}
\toprule
\# & $(A,R)$ & confluent & terminating & unique NFs \\
\midrule
1 & $(\varnothing,\varnothing)$ & True & True & True \\
2 & $(\{a\},\varnothing)$ & True & True & True \\
3 & $(\{a\},\{(a,a)\})$ & True & False & False \\
4 & $(\{a,b,c\},\{(a,b),(a,c)\})$ & False & True & False \\
5 & $(\{a,b\},\{(a,a),(a,b)\})$ & True & False & True \\
6 & $(\{a,b,c\},\{(a,b),(b,b),(a,c)\})$ & False & False & False \\
7 & $(\{a,b,c\},\{(a,b),(b,b),(a,c),(c,c)\})$ & False & False & False \\
\bottomrule
\end{tabular}
\end{center}

\textbf{All 8 Combinations:}
\begin{center}
\begin{tabular}{@{}ccccl@{}}
\toprule
confluent & terminating & unique NFs & example \\
\midrule
True  & True  & True  & ARS 2 (or 1) \\
True  & True  & False  & \textit{Impossible} \\
True  & False  & True  & ARS 5 \\
True  & False  & False  & ARS 3 \\
False & True  & True  & \textit{Impossible} \\
False & True  & False  & ARS 4 \\
False & False & True  & \textit{Impossible} \\
False & False & False & ARS 6 (or 7) \\
\bottomrule
\end{tabular}
\end{center}

\subsubsection{Questions}
\emph{Is there an easy way to tell from the graph if an ARS will terminate, or do you always have to trace every path?}

% ---------- Week 3 ----------
\subsection{Week 3}

\subsubsection{Notes and Exploration}
HW3 moved from drawing small ARS graphs to \textbf{string rewriting}. The key shift is that we are no longer just asking “what edges exist?”, but instead:
\begin{itemize}[leftmargin=1.4em]
  \item whether rewriting \emph{terminates} (i.e., there are no infinite $\to$-chains),
  \item what the \emph{normal forms} are (strings that cannot be rewritten further),
  \item and what the \emph{equivalence classes} are under $\leftrightarrow^{\!*}$ (the equality generated by rewriting).
\end{itemize}
The point is that $\leftrightarrow^{\!*}$ is the “real meaning” (what strings count as equal), while $\to$ is just one particular implementation.

\subsubsection{Homework}

\paragraph{Exercise 5.}
Rewrite rules over $\{a,b\}$:
\[
ab \to ba
\qquad
ba \to ab
\qquad
aa \to \varepsilon
\qquad
b \to \varepsilon
\]
(where $\varepsilon$ is the empty string).

\begin{enumerate}[label=\textbf{\arabic*.},leftmargin=1.6em]
  \item \textbf{Reduce some example strings (e.g.\ \texttt{abba} and \texttt{bababa}).}

  \textbf{Example 1:} \texttt{abba}.
  \[
  \texttt{abba} \to \texttt{aba} \to \texttt{aa} \to \varepsilon.
  \]
  So \texttt{abba} reduces to the normal form $\varepsilon$.

  \textbf{Example 2:} \texttt{bababa}.
  \[
  \texttt{bababa} \to \texttt{ababa} \to \texttt{aaba} \to \texttt{aaa} \to \texttt{a}.
  \]
  (One possible route: delete each $b$ using $b\to\varepsilon$, then reduce $aa\to\varepsilon$ once.)
  So \texttt{bababa} reduces to the normal form \texttt{a}.

  \item \textbf{Why is the ARS not terminating?}

  Because there is an infinite rewrite loop:
  \[
  \texttt{ab} \to \texttt{ba} \to \texttt{ab} \to \texttt{ba} \to \cdots
  \]
  So there exists an infinite $\to$-sequence, hence the system is \textbf{not terminating}.

  \item \textbf{Find two strings that are not equivalent. How many non-equivalent strings can you find?}

  Define an invariant:
  \[
  I(w) \;=\; (\#a\text{ in }w)\bmod 2.
  \]
  Check each rule preserves $I$:
  \begin{itemize}[leftmargin=1.4em]
    \item $ab\leftrightarrow ba$ does not change how many $a$'s appear.
    \item $aa \to \varepsilon$ changes the number of $a$'s by $-2$, so parity is unchanged.
    \item $b \to \varepsilon$ does not change the number of $a$'s.
  \end{itemize}
  Therefore $I(w)$ is constant on $\leftrightarrow^{\!*}$-equivalence classes.

  So $\varepsilon$ (even number of $a$'s) and \texttt{a} (odd number of $a$'s) are \textbf{not equivalent}.
  In fact, this shows there are \textbf{at least two} equivalence classes.

  \item \textbf{How many equivalence classes does $\leftrightarrow^{\!*}$ have? Describe them nicely. What are the normal forms?}

  The invariant above suggests exactly two classes:
  \begin{itemize}[leftmargin=1.4em]
    \item \textbf{Even-$a$ class:} all strings with an even number of $a$'s.
    \item \textbf{Odd-$a$ class:} all strings with an odd number of $a$'s.
  \end{itemize}

  \textbf{Why there are not more than two:}  
  From any string $w$, delete all $b$'s using $b\to\varepsilon$. This leaves a string $a^n$.  
  Then repeatedly apply $aa\to\varepsilon$:
  \[
  a^{2k} \to \varepsilon,
  \qquad
  a^{2k+1} \to a.
  \]
  So every string reduces to either $\varepsilon$ or \texttt{a}. These are irreducible (no rule applies), so they are \textbf{normal forms}.

  Therefore the normal forms are:
  \[
  \textbf{Normal forms: } \{\varepsilon,\; \texttt{a}\}.
  \]
  And the equivalence class is determined exactly by the parity of the number of $a$'s.

  \textbf{Specification (rule-free statement):} this system computes whether the number of $a$'s in the input string is \textbf{even or odd}.

  \item \textbf{Modify the ARS so it becomes terminating without changing its equivalence classes.}

  The nontermination comes from having \emph{both} swap directions. A terminating version is obtained by orienting the swap in only one direction, e.g.
  \[
  ba \to ab,
  \qquad
  aa \to \varepsilon,
  \qquad
  b \to \varepsilon.
  \]
  This is terminating because:
  \begin{itemize}[leftmargin=1.4em]
    \item each $ba\to ab$ decreases the number of “inversions” (a $b$ sitting to the left of an $a$),
    \item and $aa\to\varepsilon$, $b\to\varepsilon$ strictly decrease length.
  \end{itemize}
  The equivalence classes under $\leftrightarrow^{\!*}$ stay the same, because $\leftrightarrow^{\!*}$ already treats rules as reversible (it includes inverse steps), so allowing one swap direction is enough to generate the same notion of “same multiset up to swaps,” together with the same deletions/insertions of $aa$ and $b$.

  \item \textbf{Write down a question or two about strings that can be answered using the ARS.}

  Since the invariant is $(\#a)\bmod 2$, good “semantic” questions are:
  \begin{itemize}[leftmargin=1.4em]
    \item “Is this string equivalent to $\varepsilon$?” (i.e.\ does it have an even number of $a$'s?)
    \item “Are two strings equivalent?” (i.e.\ do they have the same parity of $a$'s?)
  \end{itemize}
\end{enumerate}

\bigskip
\paragraph{Exercise 5b.}
Same as Exercise 5, but replace $aa\to\varepsilon$ with $aa\to a$:
\[
ab \to ba
\qquad
ba \to ab
\qquad
aa \to a
\qquad
b \to \varepsilon
\]

\begin{enumerate}[label=\textbf{\arabic*.},leftmargin=1.6em]
  \item \textbf{Reduce the example strings again.}

  \textbf{Example 1:} \texttt{abba}.
  \[
  \texttt{abba} \to \texttt{aba} \to \texttt{aa} \to \texttt{a}.
  \]
  So the normal form is \texttt{a}.

  \textbf{Example 2:} \texttt{bababa}.
  Delete $b$'s to get \texttt{aaa}, then contract:
  \[
  \texttt{bababa} \to \texttt{aaa} \to \texttt{aa} \to \texttt{a}.
  \]
  So the normal form is again \texttt{a}.

  \item \textbf{Why is the ARS not terminating?}

  The same infinite loop still exists:
  \[
  \texttt{ab} \to \texttt{ba} \to \texttt{ab} \to \cdots
  \]
  Hence \textbf{not terminating}.

  \item \textbf{Find two strings that are not equivalent; how many non-equivalent strings can you find?}

  Here parity is \emph{not} invariant anymore, because $aa\to a$ changes $\#a$ by $-1$.

  A correct invariant is:
  \[
  J(w) \;=\; \text{“does $w$ contain at least one $a$?”}
  \]
  Check:
  \begin{itemize}[leftmargin=1.4em]
    \item swaps do not create/destroy $a$'s,
    \item $b\to\varepsilon$ does not affect whether an $a$ exists,
    \item $aa\to a$ never removes the \emph{last} $a$ (it turns two $a$'s into one $a$).
  \end{itemize}
  So “having an $a$” is invariant under $\leftrightarrow^{\!*}$.

  Therefore $\varepsilon$ (no $a$) and \texttt{a} (has an $a$) are \textbf{not equivalent}, so there are at least two equivalence classes.

  \item \textbf{How many equivalence classes are there? What are the normal forms?}

  Every string either has no $a$ (only $b$'s), or has at least one $a$.
  \begin{itemize}[leftmargin=1.4em]
    \item If $w$ has no $a$, then repeatedly delete $b$'s to reach $\varepsilon$.
    \item If $w$ has at least one $a$, delete all $b$'s, leaving $a^n$ with $n\ge 1$, then repeatedly apply $aa\to a$ until only \texttt{a} remains.
  \end{itemize}
  So again the normal forms are:
  \[
  \textbf{Normal forms: } \{\varepsilon,\; \texttt{a}\}.
  \]
  But now they represent two different meanings:
  \begin{itemize}[leftmargin=1.4em]
    \item $\varepsilon$ = “there are no $a$'s in the input,”
    \item \texttt{a} = “there is at least one $a$ in the input.”
  \end{itemize}

  \textbf{Specification (rule-free statement):} this system computes whether the input string contains an $a$ or not.

  \item \textbf{Modify the ARS to become terminating without changing equivalence classes.}

  As before, orient the swap one way:
  \[
  ba \to ab,
  \qquad
  aa \to a,
  \qquad
  b \to \varepsilon.
  \]
  Termination follows from the same measure idea (inversions decrease under $ba\to ab$ and length decreases under $aa\to a$ and $b\to\varepsilon$), while $\leftrightarrow^{\!*}$-equivalence remains the same.

  \item \textbf{Questions answerable using the ARS.}

  Since the invariant is “has an $a$,” natural questions are:
  \begin{itemize}[leftmargin=1.4em]
    \item “Is this string equivalent to $\varepsilon$?” (i.e.\ does it have no $a$?)
    \item “Are two strings equivalent?” (i.e.\ do they either both have an $a$, or both have none?)
  \end{itemize}
\end{enumerate}

\subsubsection{Questions}
\emph{Could we make a version of 5b that is still non-terminating but somehow keeps unique normal forms?}

% ---------- Week 4 ----------
\subsection{Week 4}

\subsubsection{Notes and Exploration}
This week we focused on \textbf{termination proofs} using \textbf{measure functions}.
The general pattern is:
\begin{itemize}[leftmargin=1.4em]
  \item pick a function $\varphi(\text{state})$ that maps every program state to a value in a well-founded set (usually $\mathbb{N}$),
  \item show $\varphi$ is always $\ge 0$ (so it cannot decrease forever), and
  \item show every loop iteration (or recursive call) makes $\varphi$ strictly smaller.
\end{itemize}
If both are true, then an infinite execution would force an infinite strictly-decreasing sequence in $\mathbb{N}$, which is impossible. So the program must terminate.

\subsubsection{Homework}

\paragraph{HW 4.1 (Euclidean Algorithm).}
\textbf{Algorithm.}
\begin{verbatim}
while b != 0:
  temp = b
  b = a mod b
  a = temp
return a
\end{verbatim}

\textbf{Question: Under what conditions does this always terminate?}

A clean set of conditions is:
\[
a,b \in \mathbb{N} \quad \text{(natural numbers),}
\]
and we interpret $a \bmod b$ in the standard way when $b>0$, meaning:
\[
0 \le (a \bmod b) < b.
\]
(Also note: if $b=0$ at the start, the loop does zero iterations and the algorithm terminates immediately.)

\textbf{Measure function.}
Define the measure on program states $(a,b)$ by
\[
\varphi(a,b) = b.
\]

\textbf{Proof of termination.}
Assume we are in a loop iteration, so the guard is true and therefore $b \ne 0$, i.e.\ $b>0$ in $\mathbb{N}$.

Inside the loop we assign
\[
b \;:=\; a \bmod b.
\]
By the defining property of the remainder when $b>0$,
\[
0 \le a \bmod b < b.
\]
So after the assignment, the new value of $b$ (call it $b'$) satisfies:
\[
0 \le b' < b.
\]
That means the measure strictly decreases each iteration:
\[
\varphi(a',b') = b' < b = \varphi(a,b).
\]

Also, $\varphi(a,b)=b$ is always a natural number, so it is bounded below by $0$ and cannot decrease forever.
Therefore the loop can only run finitely many times, so the algorithm terminates.

\textbf{Conclusion.}
Under the condition that $a,b\in\mathbb{N}$ and $\bmod$ is the standard remainder operation (so $0\le a\bmod b < b$ for $b>0$), the algorithm always terminates.

\bigskip

\paragraph{HW 4.2 (Merge Sort Fragment).}
\textbf{Code fragment.}
\begin{verbatim}
function merge_sort(arr, left, right):
  if left >= right:
    return
  mid = (left + right) / 2
  merge_sort(arr, left, mid)
  merge_sort(arr, mid+1, right)
  merge(arr, left, mid, right)
\end{verbatim}

\textbf{Claim.} The function
\[
\varphi(left,right) = right - left + 1
\]
is a measure function for \texttt{merge\_sort}.

\textbf{Why this is a valid measure.}
When \texttt{merge\_sort} actually recurses, it is in the case $left < right$ (since if $left \ge right$ it returns immediately).
So in the recursive case we have $right-left \ge 1$, which implies:
\[
\varphi(left,right) = right-left+1 \ge 2.
\]
In particular, $\varphi(left,right)\in\mathbb{N}$ and is always at least $1$ when $left \le right$.

\textbf{Strict decrease on recursive calls.}
Let
\[
n = \varphi(left,right)=right-left+1.
\]
In the recursive case $n\ge 2$. The midpoint $mid$ is chosen so that
\[
left \le mid < right.
\]
Now compare the measures.

\emph{First recursive call:} \texttt{merge\_sort(arr,left,mid)} has measure
\[
\varphi(left,mid) = mid-left+1.
\]
Because $mid<right$, we have $mid-left+1 \le (right-1)-left+1 = right-left = n-1$.
So
\[
\varphi(left,mid) \le n-1 < n.
\]

\emph{Second recursive call:} \texttt{merge\_sort(arr,mid+1,right)} has measure
\[
\varphi(mid+1,right) = right-(mid+1)+1 = right-mid.
\]
Because $left \le mid$, we have $right-mid \le right-left = n-1$.
So
\[
\varphi(mid+1,right) \le n-1 < n.
\]

Thus, in both recursive calls, the measure is strictly smaller than the original $n$.

\textbf{Bounded below.}
The measure $\varphi$ always takes values in $\mathbb{N}$ and (for any non-empty interval) is at least $1$.
So it cannot decrease infinitely many times.

\textbf{Conclusion.}
Every recursive call strictly decreases $\varphi(left,right)=right-left+1$, and the measure is bounded below in $\mathbb{N}$, so the recursion must terminate.

\subsubsection{Questions}
\emph{When picking a measure function, how do I decide whether to measure “size of the input” (like $right-left+1$) versus something more indirect (like number of inversions or a lexicographic pair of measures)?}


% ---------- Week 5 ----------
\subsection{Week 5}

\subsubsection{Notes and Exploration}
This week we practiced $\lambda$-calculus reduction using:
\begin{itemize}
  \item \textbf{$\alpha$-renaming} (renaming bound variables) to avoid confusion/capture.
  \item \textbf{$\beta$-reduction}: $(\lambda x.\,M)\,N \mapsto M[N/x]$ (substitute $N$ for free occurrences of $x$ in $M$).
\end{itemize}

\subsubsection{Homework}
Evaluate:
\[
(\lambda f.\lambda x.f(f(x)))\,(\lambda f.\lambda x.f(f(f(x)))).
\]

Let
\[
A := \lambda f.\lambda x.\, f(f(x))
\qquad\text{and}\qquad
B := \lambda f.\lambda x.\, f(f(f(x))).
\]
We compute \(A\,B\).

\medskip
\textbf{Step 1 (\(\beta\)-reduction).}
\[
(\lambda f.\lambda x.\, f(f(x)))\,B
\;\mapsto_{\beta}\;
\lambda x.\, B(B(x)).
\]
So it remains to reduce \(B(B(x))\).

\medskip
\textbf{Step 2 (reduce \(B(x)\)).}
\[
B(x)
=
(\lambda f.\lambda x.\, f(f(f(x))))\,x.
\]
To avoid confusion with two different binders named \(x\), rename the inner bound variable \(x\) to \(u\) (\(\alpha\)-conversion):
\[
(\lambda f.\lambda u.\, f(f(f(u))))\,x
\;\mapsto_{\beta}\;
\lambda u.\, x(x(x(u))).
\]
Call this result
\[
G := \lambda u.\, x(x(x(u))).
\]
So \(B(x) \mapsto G\).

\medskip
\textbf{Step 3 (now reduce \(B(G)\)).}
\[
B(G)
=
(\lambda f.\lambda x.\, f(f(f(x))))\,G
\;\mapsto_{\beta}\;
\lambda x.\, G(G(G(x))).
\]
Again rename the bound variable \(x\) to \(y\) to keep symbols distinct:
\[
B(G) \mapsto \lambda y.\, G(G(G(y))).
\]

\medskip
\textbf{Step 4 (expand what \(G\) does).}
Recall \(G(t) = x(x(x(t)))\). Then:
\[
G(y) = x^3(y),
\]
\[
G(G(y)) = G(x^3(y)) = x^3(x^3(y)) = x^6(y),
\]
\[
G(G(G(y))) = G(x^6(y)) = x^3(x^6(y)) = x^9(y).
\]
So
\[
\lambda y.\, G(G(G(y))) \;=\; \lambda y.\, x^9(y).
\]

\medskip
\textbf{Final answer.}
Putting this back into Step 1:
\[
A\,B \;\mapsto\; \lambda x.\, B(B(x))
\;\mapsto\; \lambda x.\, (\lambda y.\, x^9(y)).
\]
Equivalently, this is a function that takes \(x\) and returns the function \(y \mapsto x^9(y)\).

\subsubsection{Questions}
\emph{What would happen if we swapped the two functions in the workout?}

% ---------- Week 6 ----------
\subsection{Week 6}

\subsubsection{Notes and Exploration}
We computed factorial using the fixed-point combinator \texttt{fix}, and we practiced reducing
\texttt{let rec} by rewriting it into \texttt{fix} and then applying \(\beta\)-reduction step by step.

\subsubsection{Homework}
Compute:
\[
\texttt{let rec fact = \textbackslash n.\ if n=0 then 1 else n * fact (n-1) in fact 3}.
\]

We use the computation rules:
\[
\texttt{fix F} \mapsto \texttt{F (fix F)}, \qquad
\texttt{let x = e1 in e2} \mapsto (\lambda x.\,e2)\,e1,
\]
\[
\texttt{let rec f = e1 in e2} \mapsto \texttt{let f = (fix (\textbackslash f.\,e1)) in e2}.
\]
We also use the given computation rules for \texttt{if}, equality with \(0\), and basic arithmetic
(e.g. \(3=0\mapsto \texttt{False}\), \(0=0\mapsto \texttt{True}\), \(\texttt{if True then A else B}\mapsto A\),
\(\texttt{if False then A else B}\mapsto B\), and \(3-1\mapsto 2\), etc.).

\medskip
\noindent
Let
\[
F \;:=\; (\lambda \texttt{fact}.\ \lambda n.\ \texttt{if } n=0 \texttt{ then } 1 \texttt{ else } n * \texttt{fact}(n-1)).
\]

\medskip
\noindent\textbf{Start:}
\[
E_0 \;:=\; \texttt{let rec fact = \textbackslash n.\ if n=0 then 1 else n * fact (n-1) in fact 3}.
\]

\medskip
\noindent\textbf{Step-by-step reduction:}
\begin{align*}
E_0
&\mapsto \texttt{let fact = (fix (\textbackslash fact.\ \textbackslash n.\ if n=0 then 1 else n * fact (n-1))) in fact 3}
&&\langle \text{def of let rec} \rangle \\[4pt]
&\mapsto (\lambda \texttt{fact}.\ \texttt{fact}\ 3)\ (\texttt{fix }F)
&&\langle \text{def of let} \rangle \\[4pt]
&\mapsto (\texttt{fix }F)\ 3
&&\langle \beta \rangle \\[8pt]
&\mapsto (F(\texttt{fix }F))\ 3
&&\langle \text{def of fix} \rangle \\[4pt]
&\mapsto (\lambda n.\ \texttt{if } n=0 \texttt{ then } 1 \texttt{ else } n * (\texttt{fix }F)(n-1))\ 3
&&\langle \beta \rangle \\[4pt]
&\mapsto \texttt{if } 3=0 \texttt{ then } 1 \texttt{ else } 3 * (\texttt{fix }F)(3-1)
&&\langle \beta \rangle \\[4pt]
&\mapsto \texttt{if False then } 1 \texttt{ else } 3 * (\texttt{fix }F)(3-1)
&&\langle \text{def of } (=0) \rangle \\[4pt]
&\mapsto 3 * (\texttt{fix }F)(3-1)
&&\langle \text{def of if} \rangle \\[4pt]
&\mapsto 3 * (\texttt{fix }F)\ 2
&&\langle \text{arith: }3-1\mapsto 2 \rangle \\[10pt]
&\mapsto 3 * (F(\texttt{fix }F))\ 2
&&\langle \text{def of fix} \rangle \\[4pt]
&\mapsto 3 * (\lambda n.\ \texttt{if } n=0 \texttt{ then } 1 \texttt{ else } n * (\texttt{fix }F)(n-1))\ 2
&&\langle \beta \rangle \\[4pt]
&\mapsto 3 * (\texttt{if } 2=0 \texttt{ then } 1 \texttt{ else } 2 * (\texttt{fix }F)(2-1))
&&\langle \beta \rangle \\[4pt]
&\mapsto 3 * (\texttt{if False then } 1 \texttt{ else } 2 * (\texttt{fix }F)(2-1))
&&\langle \text{def of } (=0) \rangle \\[4pt]
&\mapsto 3 * (2 * (\texttt{fix }F)(2-1))
&&\langle \text{def of if} \rangle \\[4pt]
&\mapsto 3 * (2 * (\texttt{fix }F)\ 1)
&&\langle \text{arith: }2-1\mapsto 1 \rangle \\[10pt]
&\mapsto 3 * (2 * (F(\texttt{fix }F))\ 1)
&&\langle \text{def of fix} \rangle \\[4pt]
&\mapsto 3 * (2 * (\lambda n.\ \texttt{if } n=0 \texttt{ then } 1 \texttt{ else } n * (\texttt{fix }F)(n-1))\ 1)
&&\langle \beta \rangle \\[4pt]
&\mapsto 3 * (2 * (\texttt{if } 1=0 \texttt{ then } 1 \texttt{ else } 1 * (\texttt{fix }F)(1-1)))
&&\langle \beta \rangle \\[4pt]
&\mapsto 3 * (2 * (\texttt{if False then } 1 \texttt{ else } 1 * (\texttt{fix }F)(1-1)))
&&\langle \text{def of } (=0) \rangle \\[4pt]
&\mapsto 3 * (2 * (1 * (\texttt{fix }F)(1-1)))
&&\langle \text{def of if} \rangle \\[4pt]
&\mapsto 3 * (2 * (1 * (\texttt{fix }F)\ 0))
&&\langle \text{arith: }1-1\mapsto 0 \rangle \\[10pt]
&\mapsto 3 * (2 * (1 * (F(\texttt{fix }F))\ 0))
&&\langle \text{def of fix} \rangle \\[4pt]
&\mapsto 3 * (2 * (1 * (\lambda n.\ \texttt{if } n=0 \texttt{ then } 1 \texttt{ else } n * (\texttt{fix }F)(n-1))\ 0))
&&\langle \beta \rangle \\[4pt]
&\mapsto 3 * (2 * (1 * (\texttt{if } 0=0 \texttt{ then } 1 \texttt{ else } 0 * (\texttt{fix }F)(0-1))))
&&\langle \beta \rangle \\[4pt]
&\mapsto 3 * (2 * (1 * (\texttt{if True then } 1 \texttt{ else } 0 * (\texttt{fix }F)(0-1))))
&&\langle \text{def of } (=0) \rangle \\[4pt]
&\mapsto 3 * (2 * (1 * 1))
&&\langle \text{def of if} \rangle \\[8pt]
&\mapsto 3 * (2 * 1)
&&\langle \text{arith: }1*1\mapsto 1 \rangle \\[4pt]
&\mapsto 3 * 2
&&\langle \text{arith: }2*1\mapsto 2 \rangle \\[4pt]
&\mapsto 6
&&\langle \text{arith: }3*2\mapsto 6 \rangle
\end{align*}

\textbf{Conclusion.} \\
By repeatedly expanding \texttt{fix} (to unfold recursion) and then applying \(\beta\)-reduction and the
\texttt{if}/arithmetic rules, the term reduces to \(\mathbf{6}\). So \(\texttt{fact 3}\) computes \(3!\).

\subsubsection{Questions}
\emph{Would using $\alpha$-conversion anywhere in this computation change the result, or just make it cleaner?}


% ---------- Week 7 ----------
\subsection{Week 7}

\subsubsection{Notes and Exploration}
Parsing and context-free grammars. We use the grammar (nonterminals \texttt{Exp}, \texttt{Exp1}, \texttt{Exp2}, and \texttt{Int}):

\begin{verbatim}
Exp  -> Exp '+' Exp1
Exp  -> Exp1
Exp1 -> Exp1 '*' Exp2
Exp1 -> Exp2
Exp2 -> Int
Exp2 -> '(' Exp ')'
Int  -> 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
\end{verbatim}

\subsubsection{Homework: Parse Trees}
Below are derivation trees (concrete syntax trees) for the required expressions.
Nonterminals are boxed; terminals are leaves.
(\textbf{Note:} Integers are introduced using the \texttt{Int}-rule, i.e.\ \texttt{Exp2 -> Int -> digit}.)

% ---------------------------------------------------------
\paragraph{(a) \texttt{2+1}}
\begin{center}
\begin{tikzpicture}[ptree]
\node[nt]{Exp}
  child { node[nt]{Exp}
    child { node[nt]{Exp1}
      child { node[nt]{Exp2}
        child { node[nt]{Int}
          child { node[tm]{2} }
        }
      }
    }
  }
  child { node[tm]{+} }
  child { node[nt]{Exp1}
    child { node[nt]{Exp2}
      child { node[nt]{Int}
        child { node[tm]{1} }
      }
    }
  };
\end{tikzpicture}
\end{center}

% ---------------------------------------------------------
\paragraph{(b) \texttt{1+2*3}}
\begin{center}
\begin{tikzpicture}[ptree]
\node[nt]{Exp}
  child { node[nt]{Exp}
    child { node[nt]{Exp1}
      child { node[nt]{Exp2}
        child { node[nt]{Int}
          child { node[tm]{1} }
        }
      }
    }
  }
  child { node[tm]{+} }
  child { node[nt]{Exp1}
    child { node[nt]{Exp1}
      child { node[nt]{Exp2}
        child { node[nt]{Int}
          child { node[tm]{2} }
        }
      }
    }
    child { node[tm]{*} }
    child { node[nt]{Exp2}
      child { node[nt]{Int}
        child { node[tm]{3} }
      }
    }
  };
\end{tikzpicture}
\end{center}

% ---------------------------------------------------------
\paragraph{(c) \texttt{1+(2*3)}}
\begin{center}
\begin{tikzpicture}[ptree]
\node[nt]{Exp}
  child { node[nt]{Exp}
    child { node[nt]{Exp1}
      child { node[nt]{Exp2}
        child { node[nt]{Int}
          child { node[tm]{1} }
        }
      }
    }
  }
  child { node[tm]{+} }
  child { node[nt]{Exp1}
    child { node[nt]{Exp2}
      child { node[tm]{(} }
      child { node[nt]{Exp}
        child { node[nt]{Exp1}
          child { node[nt]{Exp1}
            child { node[nt]{Exp2}
              child { node[nt]{Int}
                child { node[tm]{2} }
              }
            }
          }
          child { node[tm]{*} }
          child { node[nt]{Exp2}
            child { node[nt]{Int}
              child { node[tm]{3} }
            }
          }
        }
      }
      child { node[tm]{)} }
    }
  };
\end{tikzpicture}
\end{center}

% ---------------------------------------------------------
\paragraph{(d) \texttt{(1+2)*3}}
\begin{center}
\begin{tikzpicture}[ptree]
\node[nt]{Exp}
  child { node[nt]{Exp1}
    child { node[nt]{Exp1}
      child { node[nt]{Exp2}
        child { node[tm]{(} }
        child { node[nt]{Exp}
          child { node[nt]{Exp}
            child { node[nt]{Exp1}
              child { node[nt]{Exp2}
                child { node[nt]{Int}
                  child { node[tm]{1} }
                }
              }
            }
          }
          child { node[tm]{+} }
          child { node[nt]{Exp1}
            child { node[nt]{Exp2}
              child { node[nt]{Int}
                child { node[tm]{2} }
              }
            }
          }
        }
        child { node[tm]{)} }
      }
    }
    child { node[tm]{*} }
    child { node[nt]{Exp2}
      child { node[nt]{Int}
        child { node[tm]{3} }
      }
    }
  };
\end{tikzpicture}
\end{center}

% ---------------------------------------------------------
\paragraph{(e) \texttt{1+2*3+4*5+6}}
\begin{center}
\begin{tikzpicture}[ptree]
\node[nt]{Exp}
  % left part: (1 + 2*3) + 4*5
  child { node[nt]{Exp}
    child { node[nt]{Exp}
      % 1 + 2*3
      child { node[nt]{Exp}
        child { node[nt]{Exp1}
          child { node[nt]{Exp2}
            child { node[nt]{Int}
              child { node[tm]{1} }
            }
          }
        }
      }
      child { node[tm]{+} }
      child { node[nt]{Exp1}
        child { node[nt]{Exp1}
          child { node[nt]{Exp2}
            child { node[nt]{Int}
              child { node[tm]{2} }
            }
          }
        }
        child { node[tm]{*} }
        child { node[nt]{Exp2}
          child { node[nt]{Int}
            child { node[tm]{3} }
          }
        }
      }
    }
    child { node[tm]{+} }
    % 4*5
    child { node[nt]{Exp1}
      child { node[nt]{Exp1}
        child { node[nt]{Exp2}
          child { node[nt]{Int}
            child { node[tm]{4} }
          }
        }
      }
      child { node[tm]{*} }
      child { node[nt]{Exp2}
        child { node[nt]{Int}
          child { node[tm]{5} }
        }
      }
    }
  }
  child { node[tm]{+} }
  % + 6
  child { node[nt]{Exp1}
    child { node[nt]{Exp2}
      child { node[nt]{Int}
        child { node[tm]{6} }
      }
    }
  };
\end{tikzpicture}
\end{center}

% =========================================================
\subsubsection{Questions}
\emph{Could this grammar still work if “+” and “*” had the same priority?}


% ---------- Week 8 ----------
\subsection{Week 8}

\subsubsection{Notes and Exploration}
I completed Levels 5--8 of the \emph{Natural Number Game} (NNG). The key tools were:
\begin{itemize}[leftmargin=1.4em]
  \item \textbf{Rewriting} with \texttt{rw [lemma]} to substitute equals for equals.
  \item \textbf{\texttt{rfl}} to close a goal when both sides are definitionally identical.
  \item Peano-style equations for addition and numerals, including \texttt{add\_zero}, \texttt{add\_succ}, \texttt{add\_one}, \texttt{one\_eq\_succ\_zero}, and \texttt{two\_eq\_succ\_one}.
\end{itemize}

\subsubsection{Homework (NNG Levels 5--8)}
\paragraph{5/8: Adding zero.} \emph{Goal:} $a + (b+0) + (c+0) = a+b+c$.\\
\emph{Lean steps:} \texttt{rw [add\_zero]} (on $b+0$), \texttt{rw [add\_zero]} (on $c+0$), \texttt{rfl}.

\paragraph{6/8: Precision rewriting.} \emph{Goal:} $a + (b+0) + (c+0) = a+b+c$.\\
\emph{Lean steps:} \texttt{rw [add\_zero]} (on $b+0$), \texttt{rw [add\_zero]} (on $c+0$), \texttt{rfl}.

\paragraph{7/8: \texttt{add\_succ}.} \emph{Goal:} $\mathrm{succ}\,n = n+1$.\\
\emph{Lean steps:} \texttt{rw [one\_eq\_succ\_zero]} to get $n+1=n+\mathrm{succ}\,0$; \texttt{rw [add\_succ]} to obtain $\mathrm{succ}(n+0)$; \texttt{rw [add\_zero]}; \texttt{rfl}.

\paragraph{8/8: $2+2=4$.} \emph{Outline:} Rewrite $2$ and $4$ as successors (\texttt{two\_eq\_succ\_one}, etc.), use \texttt{add\_succ} to pull \(\mathrm{succ}\) out of addition stepwise, and finish by \texttt{rfl} once both sides match.

\subsubsection{Natural-Language Proof (English math proof)}
\paragraph{Claim.} \(2+2=4\).

\paragraph{Assumptions/Definitions.}
Let \(0\) be the base natural number and \(\mathrm{succ}(n)\) the successor of \(n\).
Define \(1=\mathrm{succ}(0)\), \(2=\mathrm{succ}(1)\), and \(4=\mathrm{succ}(\mathrm{succ}(\mathrm{succ}(\mathrm{succ}(0))))\).
Addition is defined by the Peano axioms:
\[
\forall a,\; a+0=a \quad\text{and}\quad \forall a,b,\; a+\mathrm{succ}(b)=\mathrm{succ}(a+b).
\]

\paragraph{Proof.}
We compute \(2+2\) using the addition axioms.
First rewrite \(2=\mathrm{succ}(1)\), so
\[
2+2 \;=\; 2+\mathrm{succ}(1) \;=\; \mathrm{succ}(2+1) \quad\text{(by the second axiom).}
\]
Again rewrite \(1=\mathrm{succ}(0)\) to get
\[
2+1 \;=\; 2+\mathrm{succ}(0) \;=\; \mathrm{succ}(2+0) \;=\; \mathrm{succ}(2) \quad\text{(using both axioms).}
\]
Therefore
\[
2+2 \;=\; \mathrm{succ}(2+1) \;=\; \mathrm{succ}(\mathrm{succ}(2)).
\]
Since \(2=\mathrm{succ}(1)\), we have
\[
\mathrm{succ}(\mathrm{succ}(2))=\mathrm{succ}(\mathrm{succ}(\mathrm{succ}(1)))=
\mathrm{succ}(\mathrm{succ}(\mathrm{succ}(\mathrm{succ}(0))))=4.
\]
Hence \(2+2=4\). \(\square\)

\subsubsection{Discord Question}
\emph{When using \texttt{rw [add\_zero]}, how does Lean know which part of the equation to rewrite first?}

% ---------- Week 9 ----------
\subsection{Week 9}

\subsubsection{Notes and Exploration}
This week I worked through \emph{Addition World} in the Natural Number Game.  
Key theorems proved in this world:
\begin{itemize}[leftmargin=1.4em]
  \item \textbf{zero\_add:} $0 + n = n$ (proved by induction).
  \item \textbf{succ\_add:} $\mathrm{succ}(a) + b = \mathrm{succ}(a + b)$ (proved by induction).
  \item \textbf{add\_comm:} $a + b = b + a$ (commutativity of $+$).
  \item \textbf{add\_assoc:} $(a + b) + c = a + (b + c)$ (associativity of $+$).
  \item \textbf{add\_right\_comm:} $(a + b) + c = (a + c) + b$.
\end{itemize}

The HW 9 instruction is:  
For Level 5 of addition world (\texttt{add\_right\_comm}), give two solutions in the report:
(1) a proof that uses induction and  
(2) a proof that does \emph{not} use induction.  
For each version, also write the corresponding ``pen-and-paper'' math proof.

\subsubsection{Homework (Addition World Level 5 / HW 9)}

\paragraph{Theorem (Level 5).}
For all natural numbers $a,b,c$, we have
\[
(a + b) + c \;=\; (a + c) + b.
\]
In Lean, this theorem is called \texttt{add\_right\_comm}.

\bigskip
\noindent\textbf{Solution A: With Induction.}  
We prove \texttt{add\_right\_comm} by induction on $c$.

\emph{Lean tactic steps:}
\begin{itemize}[leftmargin=1.4em]
  \item \texttt{induction c with d hd}
  \item \texttt{rw [add\_zero]}
  \item \texttt{rw [add\_zero]}
  \item \texttt{rfl}
  \item \texttt{rw [add\_succ]}
  \item \texttt{rw [succ\_add]}
  \item \texttt{rw [hd]}
  \item \texttt{rfl}
\end{itemize}

Explanation of structure (what the tactics are doing, summarized in math terms):

\emph{Base case ($c=0$).}  
Goal:
\[
(a+b)+0 \;=\; (a+0)+b.
\]
By \texttt{add\_zero}, $(a+b)+0 = a+b$, and $(a+0)+b = a+b$.  
So both sides are $a+b$. Closed by \texttt{rfl}.

\emph{Inductive step ($c = \mathrm{succ}(d)$).}  
Induction hypothesis (called \texttt{hd} in Lean):
\[
(a+b)+d \;=\; (a+d)+b.
\]
Goal:
\[
(a+b)+\mathrm{succ}(d) \;=\; (a+\mathrm{succ}(d))+b.
\]

Using the Peano definition of $+$ on the right argument,
\[
(a+b)+\mathrm{succ}(d) = \mathrm{succ}((a+b)+d)
\quad\text{and}\quad
a+\mathrm{succ}(d) = \mathrm{succ}(a+d).
\]
So the right-hand side becomes
\[
(a+\mathrm{succ}(d))+b
= (\mathrm{succ}(a+d))+b
= \mathrm{succ}\big((a+d)+b\big)
\]
using \texttt{succ\_add}, which says $\mathrm{succ}(x)+y=\mathrm{succ}(x+y)$.

Thus the goal reduces to
\[
\mathrm{succ}\big((a+b)+d\big)
=
\mathrm{succ}\big((a+d)+b\big).
\]
By the induction hypothesis, $(a+b)+d = (a+d)+b$, so the two $\mathrm{succ}(\cdot)$ terms are equal. This closes with \texttt{rfl}.

\emph{Conclusion.}  
By induction on $c$, $(a+b)+c = (a+c)+b$ holds for all $a,b,c$.

\bigskip
\noindent\textbf{Solution B: Without Induction.}  
We can also prove \texttt{add\_right\_comm} using only associativity and commutativity of addition, without \texttt{induction}.

\emph{Lean tactic steps (no induction):}
\begin{itemize}[leftmargin=1.4em]
  \item \texttt{rw [add\_assoc]}
  \item \texttt{rw [add\_comm b c]}
  \item \texttt{rw [\textbackslash leftarrow{} add\_assoc]}
  \item \texttt{rfl}
\end{itemize}

Those steps correspond exactly to the following algebra moves:
\[
(a+b)+c
\;=\;
a+(b+c)
\quad\text{(associativity, \texttt{add\_assoc})}
\]
\[
=\;
a+(c+b)
\quad\text{(commutativity on $b+c$, \texttt{add\_comm b c})}
\]
\[
=\;
(a+c)+b
\quad\text{(associativity again, undoing \texttt{add\_assoc})}.
\]
This matches the goal $(a+b)+c = (a+c)+b$.

\bigskip
\noindent\textbf{English/Pen-and-Paper Proofs for HW 9}

\paragraph{Solution A (Induction Proof in Math).}
We prove $(a+b)+c = (a+c)+b$ for all $a,b,c \in \mathbb{N}$ by induction on $c$.

\textbf{Base case:} $c=0$.  
Then
\[
(a+b)+0 = a+b
\quad\text{and}\quad
(a+0)+b = a+b,
\]
because $x+0=x$ for any $x$.  
So $(a+b)+0 = (a+0)+b$.

\textbf{Inductive step:} Assume $(a+b)+d = (a+d)+b$ for some $d$.  
We must show $(a+b)+\mathrm{succ}(d) = (a+\mathrm{succ}(d))+b$.

By the Peano definition of addition on the right argument,
\[
(a+b)+\mathrm{succ}(d) = \mathrm{succ}((a+b)+d).
\]
Also,
\[
a+\mathrm{succ}(d) = \mathrm{succ}(a+d),
\]
so
\[
(a+\mathrm{succ}(d))+b 
= (\mathrm{succ}(a+d))+b 
= \mathrm{succ}((a+d)+b),
\]
using the lemma $\mathrm{succ}(x)+y = \mathrm{succ}(x+y)$.

So it suffices to show
\[
\mathrm{succ}((a+b)+d) = \mathrm{succ}((a+d)+b),
\]
which follows from the induction hypothesis $(a+b)+d = (a+d)+b$.  
Therefore the statement holds for $\mathrm{succ}(d)$.

By induction on $c$, $(a+b)+c = (a+c)+b$ for all $a,b,c$.

\paragraph{Solution B (Algebraic / No Induction).}
We use associativity and commutativity of addition on $\mathbb{N}$.

Starting from the left-hand side,
\[
(a+b)+c = a+(b+c)
\quad\text{(associativity of $+$)},
\]
\[
= a+(c+b)
\quad\text{(commutativity of $+$)},
\]
\[
= (a+c)+b
\quad\text{(associativity of $+$ again)}.
\]
This is exactly the desired right-hand side $(a+c)+b$.  
No induction was needed.

\subsubsection{Questions}
\emph{When should I solve these problems using induction vs without?}

% ---------- Week 10 ----------
\subsection{Week 10}

\subsubsection{Notes and Exploration}
This week I finished the \textbf{Lean Logic Game} tutorial on implications, called \emph{Party Snacks}. The tutorial shows that
\begin{itemize}[leftmargin=1.4em]
  \item an implication $P \to Q$ is a function from evidence of $P$ to evidence of $Q$;
  \item conjunction $P \land Q$ is a pair made with \texttt{and\_intro};
  \item taking a function of \emph{two} inputs and turning it into a function of \emph{one} input that returns a function (currying) is exactly what Lean is doing when we write \texttt{fun x => fun y => ...}.
\end{itemize}
Levels 6--9 are the ones required for the HW, so below I spell out every little step.

\subsubsection{Homework (Party Snacks, Levels 6--9)}

% ---------------- Level 6 ----------------
\paragraph{Level 6/9 (\texttt{and\_imp}).}
\textbf{Goal.} Build
\[
C \to D \to S
\]
from the assumption
\[
h : C \land D \to S.
\]

So Lean is asking us to produce a \emph{function} that, when you hand it a $c : C$ and then a $d : D$, it can call $h$ on the \emph{pair} $(c,d)$ and get an $S$.

\textbf{Plan.}
\begin{enumerate}[label=\arabic*.,leftmargin=1.4em]
  \item Start a function that takes $c : C$.
  \item Inside it, start another function that takes $d : D$.
  \item Turn $(c,d)$ into evidence of $C \land D$ using \texttt{and\_intro c d}.
  \item Feed that to $h$ to get the $S$.
\end{enumerate}

\textbf{Step-by-step term.}
\[
\underbrace{\texttt{fun c : C =>}}_{\text{1st argument}}
\underbrace{\texttt{ fun d : D =>}}_{\text{2nd argument}}
\underbrace{\texttt{ h (and\_intro c d) }}_{\text{call the given implication}}
\]

\textbf{What Lean checks.}
\begin{itemize}[leftmargin=1.4em]
  \item After \texttt{fun c =>} the goal becomes $D \to S$ (because we promised to produce a function of $c$).
  \item After \texttt{fun d =>} the goal becomes $S$.
  \item \texttt{and\_intro c d} has type $C \land D$.
  \item $h$ has type $C \land D \to S$, so $h (and\_intro c d)$ has type $S$, which matches the goal.
\end{itemize}

\textbf{Final Lean line.}
\begin{verbatim}
exact fun c => fun d => h (and_intro c d)
\end{verbatim}

% ---------------- Level 7 ----------------
\paragraph{Level 7/9 (\texttt{and\_imp 2}).}
\textbf{Goal.} Build
\[
(C \land D) \to S
\]
from the assumption
\[
h : C \to D \to S.
\]

This is the \emph{uncurried} direction: instead of receiving $C$ and $D$ separately, we receive \emph{one} thing, namely evidence for $C \land D$, and from that we must produce $S$.

\textbf{Plan.}
\begin{enumerate}[label=\arabic*.,leftmargin=1.4em]
  \item Start a function that takes $hcd : C \land D$.
  \item From $hcd$ we can project the left part: \texttt{hcd.left} has type $C$.
  \item From $hcd$ we can project the right part: \texttt{hcd.right} has type $D$.
  \item Since $h : C \to D \to S$, we can first give it a $C$, then a $D$, to get $S$: \texttt{h (hcd.left) (hcd.right)}.
\end{enumerate}

\textbf{Step-by-step term.}
\[
\texttt{fun hcd : C ∧ D =>}
\quad
\texttt{h hcd.left hcd.right}
\]

\textbf{What Lean checks.}
\begin{itemize}[leftmargin=1.4em]
  \item After \texttt{fun hcd =>} the goal is $S$.
  \item \texttt{hcd.left} : $C$, \texttt{hcd.right} : $D$.
  \item \texttt{h hcd.left} : $D \to S$.
  \item \texttt{(h hcd.left) hcd.right} : $S$.
\end{itemize}

\textbf{Final Lean line.}
\begin{verbatim}
exact fun hcd => h hcd.left hcd.right
\end{verbatim}

% ---------------- Level 8 ----------------
\paragraph{Level 8/9 (\texttt{Distribute}).}
\textbf{Given.}
\[
h : (S \to C) \land (S \to D)
\]
So $h.left$ has type $S \to C$ and $h.right$ has type $S \to D$.

\textbf{Goal.}
\[
S \to C \land D
\]
i.e. “if we have an $s : S$, we can produce both a $C$ and a $D$.”

\textbf{Plan.}
\begin{enumerate}[label=\arabic*.,leftmargin=1.4em]
  \item Start a function that takes $s : S$.
  \item Use $h.left : S \to C$ and apply it to $s$ to get a $C$.
  \item Use $h.right : S \to D$ and apply it to $s$ to get a $D$.
  \item Glue those two into $C \land D$ using \texttt{and\_intro}.
\end{enumerate}

\textbf{Step-by-step term.}
\[
\texttt{fun s : S =>}
\quad
\texttt{and\_intro (h.left s) (h.right s)}
\]

\textbf{What Lean checks.}
\begin{itemize}[leftmargin=1.4em]
  \item After \texttt{fun s =>} the goal is $C \land D$.
  \item \texttt{h.left s} : $C$, \texttt{h.right s} : $D$.
  \item \texttt{and\_intro (h.left s) (h.right s)} : $C \land D$.
\end{itemize}

\textbf{Final Lean line.}
\begin{verbatim}
exact fun s => and_intro (h.left s) (h.right s)
\end{verbatim}

% ---------------- Level 9 ----------------
\paragraph{Level 9/9 (\texttt{Uncertain Snacks}, boss).}
\textbf{Goal.}
\[
R \to (S \to R) \land (\neg S \to R).
\]
Read: “If Riffin is bringing a snack, then (1) if Sybeth brings one, Riffin is bringing a snack, and (2) if Sybeth does \emph{not} bring one, Riffin is still bringing a snack.”

\textbf{Given.}
We start only with $r : R$.

\textbf{Plan.}
\begin{enumerate}[label=\arabic*.,leftmargin=1.4em]
  \item Start a function that takes $r : R$.
  \item We must output an \emph{and}-pair, so we will finish with \texttt{and\_intro ... ...}.
  \item Left part of the pair must have type $S \to R$.
  \item Right part of the pair must have type $(\neg S) \to R$.
  \item But we already have $r : R$, and neither of these two subgoals actually needs their argument.
  \item So for the left part we write \texttt{fun \_ : S => r}.
  \item For the right part we write \texttt{fun \_ : ¬ S => r}.
\end{enumerate}

\textbf{Step-by-step term.}
\[
\texttt{fun r : R =>}
\quad
\texttt{and\_intro}
\quad
  \underbrace{\texttt{(fun \_ : S => r)}}_{\text{has type } S \to R}
\quad
  \underbrace{\texttt{(fun \_ : ¬ S => r)}}_{\text{has type } ¬S \to R}
\]

\textbf{What Lean checks.}
\begin{itemize}[leftmargin=1.4em]
  \item After \texttt{fun r =>} the goal is $(S \to R) \land (\neg S \to R)$.
  \item \texttt{fun \_ => r} always has the right implication type, because no matter what you give it, it returns $r : R$.
  \item \texttt{and\_intro (fun \_ => r) (fun \_ => r)} has the desired conjunction type.
\end{itemize}

\textbf{Final Lean line.}
\begin{verbatim}
exact fun r => and_intro (fun _ => r) (fun _ => r)
\end{verbatim}

\subsubsection{Questions}
\emph{When is it better to package information together, and when is it better to keep it separate?}

% ---------- Week 11 ----------
\subsection{Week 11}

\subsubsection{Notes and Exploration}
I completed Levels 9--12 of the \emph{Lean Logic Negation Tutorial}. The key ideas are:
\begin{itemize}[leftmargin=1.4em]
  \item \(\neg P\) is shorthand for \(P \to \mathrm{False}\).
  \item From a conjunction \(P \land Q\), we project with \texttt{.left} and \texttt{.right}.
  \item \texttt{false\_elim} proves any proposition from \(\mathrm{False}\) (principle of explosion).
\end{itemize}

\subsubsection{Homework (Negation Tutorial Levels 9--12)}

% ---------------- Level 9 ----------------
\paragraph{Level 9/12: \textit{Allergy \#1} (\(\neg(P \land A)\) from \(h : P \to \neg A\)).}
\textbf{Goal.} \(\neg(P \land A)\). \quad
\textbf{Given.} \(h : P \to \neg A\).
\begin{itemize}[leftmargin=1.4em]
  \item Assume \(hpa : P \land A\). Then \(hpa.left : P\) and \(hpa.right : A\).
  \item From \(h\) we get \(h\,hpa.left : \neg A\), i.e.\ \(A \to \mathrm{False}\).
  \item Apply to \(hpa.right\) to reach \(\mathrm{False}\).
\end{itemize}
\textbf{Final Lean line.}
\begin{verbatim}
exact (fun hpa => (h hpa.left) hpa.right)
\end{verbatim}

% ---------------- Level 10 ----------------
\paragraph{Level 10/12: \textit{Allergy \#2} (\(P \to \neg A\) from \(h : \neg(P \land A)\)).}
\textbf{Goal.} \(P \to (A \to \mathrm{False})\). \quad
\textbf{Given.} \(h : \neg(P \land A)\).
\begin{itemize}[leftmargin=1.4em]
  \item Take \(p:P\) and \(a:A\); then \(\langle p,a\rangle : P \land A\).
  \item Apply \(h\) to derive \(\mathrm{False}\).
\end{itemize}
\textbf{Final Lean line.}
\begin{verbatim}
exact (fun p a => h ⟨p, a⟩)
\end{verbatim}

% ---------------- Level 11 ----------------
\paragraph{Level 11/12: \textit{not\_not\_not} (\(\neg A\) from \(h : \neg\neg\neg A\)).}
\textbf{Goal.} \(\neg A\). \quad
\textbf{Given.} \(h : (\neg\neg A) \to \mathrm{False}\).
\begin{itemize}[leftmargin=1.4em]
  \item To show \(A \to \mathrm{False}\), assume \(a:A\).
  \item Build \(\neg\neg A\) by \((\lambda na:\neg A,\, na\,a)\).
  \item Feed this into \(h\) to get \(\mathrm{False}\).
\end{itemize}
\textbf{Final Lean line.}
\begin{verbatim}
exact (fun a => h (fun na => na a))
\end{verbatim}

% ---------------- Level 12 ----------------
\paragraph{Level 12/12 (Boss): \(\neg\neg B\) from \(h : \neg(B \to C)\).}
\textbf{Goal.} \(\neg\neg B\), i.e.\ \((\neg B) \to \mathrm{False}\). \quad
\textbf{Given.} \(h : (B \to C) \to \mathrm{False}\).
\begin{itemize}[leftmargin=1.4em]
  \item Assume \(nb : \neg B\). Define \(f : B \to C\) by \(f\,b := \texttt{false\_elim}(nb\,b)\).
  \item Then \(h\,f : \mathrm{False}\). Hence \((\neg B) \to \mathrm{False}\).
\end{itemize}
\textbf{Final Lean line.}
\begin{verbatim}
exact (fun nb => h (fun b => false_elim (nb b)))
\end{verbatim}

\subsubsection{Questions}
\emph{Is there a quick rule of thumb for spotting when a goal is really “prove a function to False”?}

% ---------- Week 12 ----------
\subsection{Week 12}

\subsubsection{Notes and Exploration}
We read the Towers of Hanoi notes and treated the recursive definition of \texttt{hanoi} as a pair of rewrite rules.  
The two rules are

\[
\texttt{hanoi 1 x y = move x y}
\]
\[
\texttt{hanoi (n+1) x y =}
\quad
\texttt{hanoi n x (other x y);}
\quad
\texttt{move x y;}
\quad
\texttt{hanoi n (other x y) y.}
\]

I tried to think of \texttt{hanoi n x y} as the phrase “move a tower of size \(n\) from peg \(x\) to peg \(y\) using the remaining peg.”  
The trace then becomes a long horizontal and vertical picture of the recursive calls, where indentation shows call depth and each \texttt{move} line is an actual step in time.

\subsubsection{Homework (Towers of Hanoi)}

\paragraph{1. Completed execution for \texttt{hanoi 5 0 2}.}

\begin{verbatim}
hanoi 5 0 2
  hanoi 4 0 1
    hanoi 3 0 2
      hanoi 2 0 1
        hanoi 1 0 2 = move 0 2
        move 0 1
        hanoi 1 2 1 = move 2 1
      move 0 2
      hanoi 2 1 2
        hanoi 1 1 0 = move 1 0
        move 1 2
        hanoi 1 0 2 = move 0 2
    move 0 1
    hanoi 3 2 1
      hanoi 2 2 0
        hanoi 1 2 1 = move 2 1
        move 2 0
        hanoi 1 1 0 = move 1 0
      move 2 1
      hanoi 2 0 1
        hanoi 1 0 2 = move 0 2
        move 0 1
        hanoi 1 2 1 = move 2 1
  move 0 2
  hanoi 4 1 2
    hanoi 3 1 0
      hanoi 2 1 2
        hanoi 1 1 0 = move 1 0
        move 1 2
        hanoi 1 0 2 = move 0 2
      move 1 0
      hanoi 2 2 0
        hanoi 1 2 1 = move 2 1
        move 2 0
        hanoi 1 1 0 = move 1 0
    move 1 2
    hanoi 3 0 2
      hanoi 2 0 1
        hanoi 1 0 2 = move 0 2
        move 0 1
        hanoi 1 2 1 = move 2 1
      move 0 2
      hanoi 2 1 2
        hanoi 1 1 0 = move 1 0
        move 1 2
        hanoi 1 0 2 = move 0 2
\end{verbatim}

Every time the rule for \(\texttt{hanoi (n+1) x y}\) fires, the trace splits into a smaller call that uses the helper peg, then a single \texttt{move}, then another smaller call that finishes the job.

\paragraph{2. Moves extracted from the trace.}

Reading only the \texttt{move} lines from top to bottom gives the sequence of moves that solves the puzzle with five disks, from peg \(0\) to peg \(2\):

\begin{enumerate}[leftmargin=1.4em]
  \item move disk from peg 0 to peg 2
  \item move disk from peg 0 to peg 1
  \item move disk from peg 2 to peg 1
  \item move disk from peg 0 to peg 2
  \item move disk from peg 1 to peg 0
  \item move disk from peg 1 to peg 2
  \item move disk from peg 0 to peg 2
  \item move disk from peg 0 to peg 1
  \item move disk from peg 2 to peg 1
  \item move disk from peg 2 to peg 0
  \item move disk from peg 1 to peg 0
  \item move disk from peg 2 to peg 1
  \item move disk from peg 0 to peg 2
  \item move disk from peg 0 to peg 1
  \item move disk from peg 2 to peg 1
  \item move disk from peg 0 to peg 2
  \item move disk from peg 1 to peg 0
  \item move disk from peg 1 to peg 2
  \item move disk from peg 0 to peg 2
  \item move disk from peg 1 to peg 0
  \item move disk from peg 2 to peg 1
  \item move disk from peg 2 to peg 0
  \item move disk from peg 1 to peg 0
  \item move disk from peg 1 to peg 2
  \item move disk from peg 0 to peg 2
  \item move disk from peg 0 to peg 1
  \item move disk from peg 2 to peg 1
  \item move disk from peg 0 to peg 2
  \item move disk from peg 1 to peg 0
  \item move disk from peg 1 to peg 2
  \item move disk from peg 0 to peg 2
\end{enumerate}

There are \(2^{5}-1 = 31\) moves, which matches the usual formula for the minimal number of moves with five disks.

\paragraph{3. Online verification.}

I checked this list of moves against the interactive Towers of Hanoi link from the notes.  
Entering the moves in order really does move the entire stack of five disks from peg \(0\) to peg \(2\) without ever placing a larger disk on top of a smaller one, and the game also reports that the puzzle was solved in thirty one moves.  
So the completed execution trace and the extracted move list are consistent with the online version.

\subsubsection{Questions}
\emph{How can I formally prove that the Towers of Hanoi algorithm always takes exactly two to the n minus one moves?}

% ---------- Week 13 ----------
\subsection{Week 13}

\subsubsection{Notes and Exploration}
In the Hanoi notes, we treated the recursive definition of \texttt{hanoi} as two rewrite rules:
\[
\texttt{hanoi 1 x y = move x y}
\]
\[
\texttt{hanoi (n+1) x y =}
\quad
\texttt{hanoi n x (other x y);}
\quad
\texttt{move x y;}
\quad
\texttt{hanoi n (other x y) y.}
\]
To prove the move-count formula, the key idea is: \emph{count how many \texttt{move} lines these rewrite rules generate.}
This naturally gives a recurrence, and then we solve it by induction.

\subsubsection{Homework}

\textbf{Claim.}  
Let $T(n)$ be the number of \texttt{move} commands produced by the trace of \texttt{hanoi n x y} (for any pegs $x,y$).  
Then for all $n \ge 1$,
\[
T(n) = 2^{n}-1.
\]

\textbf{Step 1 (Base case from the rewrite rule).}  
For $n=1$, the rule says:
\[
\texttt{hanoi 1 x y = move x y.}
\]
So the trace has exactly one \texttt{move} line. Therefore
\[
T(1)=1,
\]
and this matches the formula because $2^{1}-1=1$.

\textbf{Step 2 (Build the recurrence by counting moves in the second rule).}  
For $n+1$, the rule expands \texttt{hanoi (n+1) x y} into three parts:
\begin{enumerate}[leftmargin=1.4em]
  \item \texttt{hanoi n x (other x y)} \quad contributes $T(n)$ moves,
  \item \texttt{move x y} \quad contributes $1$ move,
  \item \texttt{hanoi n (other x y) y} \quad contributes $T(n)$ moves.
\end{enumerate}
So the total number of moves is
\[
T(n+1)=T(n)+1+T(n)=2T(n)+1.
\]

\textbf{Step 3 (Induction proof).}  
We prove $T(n)=2^{n}-1$ for all $n\ge 1$ by induction on $n$.

\emph{Base case ($n=1$).}  
Already shown: $T(1)=1=2^{1}-1$.

\emph{Inductive step.}  
Assume as the induction hypothesis that for some $n\ge 1$,
\[
T(n)=2^{n}-1.
\]
Using the recurrence from Step 2,
\[
T(n+1)=2T(n)+1.
\]
Substitute the hypothesis:
\[
T(n+1)=2(2^{n}-1)+1=2^{n+1}-2+1=2^{n+1}-1.
\]
This is exactly the desired formula for $n+1$.

\textbf{Conclusion.}  
By induction, for every $n\ge 1$, the trace of \texttt{hanoi n x y} contains exactly $2^{n}-1$ \texttt{move} lines.

\subsubsection{Questions}
\emph{Does it matter which pegs are called 0,1,2, or is the move count always the same?}

% =========================================================
\section{Essay (Synthesis)}
How to Prove A Program Terminates\\

  The success of my programs this semester and even before this class has largely been reliant on it producing the right 
answer. Me running the program and seeing an output that matches what I expected. I learned that a program can also be
judged based on if it is guaranteed to finish. This has come up in the past, in infinite while loops in simple programming 
for example, but throughout this semester, I thought of termination in a new way. If you can identify a quantity that must
decrease every time the program takes another step, the program must terminate at some point.

  While often times it is acceptable to assume a program will terminate eventually, proving it gave me a fuller understanding 
of the program at large. I relate it with some more abstract math classes I've taken. For example, I know that 1 + 1 = 2, but
can I prove it? In the same way, I can write a program that works with my inputs but will it always work? Have I really covered
every base? 


  The basic technique for proving termination is a measure function. This function maps every program state to a number. you
then must show that every iteration or step makes the measure smaller. If every state of the program has a number 1, 2, 3, etc.
and every step makes the measure smaller, the program must eventually reach 0 and terminate.


  One example of this is the towers of Hanoi problem. The state of the computation includes the number of disks n which
is all we need to track for termination. The measure function can just be n. Basically, how many disks are left to move? 
In the recursive rule, hanoi (n+1) x y calls hanoi n x (other x y) and later hanoi n (other x y). In both cases, the value of n
goes down by 1. By the logic laid out in the paragraph above. We have a number of disks n which is a natural number and 
it decreases by 1 every step. At some point it will reach the base case 1, proving that the hanoi problem terminates.

  Another example from the semester where a measure function makes termination feel obvious is merge sort. In merge sort, the 
“state” you care about for termination is just the current interval you are sorting, which is determined by the endpoints. 
A natural measure is the length of that interval, meaning how many elements are in the section you are still responsible for sorting.
Each time merge sort recurses, it splits the interval into two smaller intervals, so the length of the problem you pass into each 
recursive call is strictly smaller than the one you started with. Since the interval length is always a natural number and cannot 
decrease forever, the recursion can only happen a finite number of times. Eventually, the interval becomes size 1 (or empty), which 
is exactly the base case where the function stops calling itself, so the algorithm must terminate.

  Overall, learning how to prove termination changed the way I evaluate programs. Instead of only checking whether something seems 
to work on a few test cases, I now think about whether the program is guaranteed to finish and why. The idea of choosing a simple 
measure and showing that it must decrease helped me see recursion and looping as something structured and predictable instead of 
something that could “run forever” if I missed a detail. Using examples like Towers of Hanoi and merge sort made the concept feel 
practical, because in both cases you can clearly identify what is getting smaller at every step until the base case is reached. 
Even if I do not write formal termination proofs in everyday coding, this mindset makes me more careful and more confident that 
I understand what my program is really doing.
\\
\\
\\
% =========================================================
\section{Evidence of Participation}
1. Creating your own programming language - Computerphile 
(https://www.youtube.com/watch?v=Q2UDHY5as90)
  In this video, Dr Laurie Tratt makes a very basic programming language. He doesn't want to make it too small it won't be easy to
tell what he is trying to do, if it's too big, it will be too advanced for the scope of the video. He starts by making an interpreter,
a calculator. He uses reverse polish notation. It allows us to find a way around lots of weird rules in math as we know it. He 
manipulates his numbers and math notation on a stack. He uses split, whcih is a python helper that helps him seperate each element 
by whitespace. It also helps to split on newlines. He then adds variables. Defining them and reading them. He does this by using 
dictionaries. Key value pairs. His end goal is to implement a factorial function. To implement comparisons he uses 0s and 1s to work 
as booleans.
The main question I had at the end of this video was how different would it be to implement a for loop compared to a while loop?

2. The Problem With A.I. Slop! - Computerphile
(https://www.youtube.com/watch?v=vrTrOCQZoQE)
In this video, Mike Pound is talking about the "wonderful world of AI slop." He talks about the fact that roughly half of the articles 
uploaded to the internet right now are AI-generated.
He starts with the why. Why are people making so many AI articles and online content. Money is the main idea.
He talks about how recipe websites are made. How the website could just be the ingredients and steps, but rather there are pictures and 
stories, possibly about the recipe being passed down. The idea of the story is that if it's been passed down through a family, it was good 
enough for them, and means something to them, so it might be the same for me. AI takes away the legitimacy of this. It can create a perfectly 
written story about how this is somebody's grandmas recipe that is totally untrue. This has come at almost zero cost to the person who made 
the site but will slowly bring in money with ads.
He believes that the state of the world wide web is in danger with the shear amount of fake news being put out by AI for financial or political 
gain. This will have an affect on things like accurate autofill search engines.
He talks about this cycle of misinformation coming from AI because as AI keeps putting information, some of it false, into the internet, 
other LLMs will use that information to train themselves. If that information is wrong, then the training data for these new LLMs will be wrong. 
These LLMs will then put less true information out, which will start a cycle causing these LLMs and the internet as a whole to be less and less 
trustworthy.
Question: What is an easy way we can tell, besides scraping website for confirmed expert human written articles, that an article is AI slop or not? 

3. Programming Loops vs Recursion - Computerphile
(https://www.youtube.com/watch?v=HXNhEYqFo0o)
In this video, Professor Brailsford explains the differences between Loops and recursion. he starts by talking about the early days of loops. 
Specifically for loops. Mostly on assembly code in the 40s and 50s. One of the first high-level languages to introduce loops was Fortran.
They were called Do loops. It talks about doing lines of code until you reach the statement with a label that you identify. Then it returns to 
the top and increments the loop counter. These can be nested as well. Nested loops are alright but they take very long to run very quickly. 
Every nested for loop goes n, n^2, n^3...
Fortran did not do user-level recursion. User Level recursion is very important for extrememly difficult multi level problems.
Algol was a language that had user level recursion. It was very slow but it came out with the right answer.
Question: What is the tipping point where recursion becomes better than loops?

4. Programming Paradigms - Computerphile
(https://www.youtube.com/watch?v=sqV3pL5x8PI)
In this video, Computer Scientist Laurence Day compares imperative vs functional programming. To add to 10 in imperative coding, for example JAVA, you start with an
integer total = 0. In imperative programming you can use a for loop to add numbers i (1...10) to total until you reach 10 where you can exit the loop.
He talks about the differences = and ==. (assignment vs equality). The main method of computation is the assignment of values to 
variables. You refer to variables that are defined globally.
Functional programming. (Haskell) "Appears to be magic" ALl you do is sum[1...10] that's it. There is a libraryu function that explains how sum
works. Sum is the function that takes a list of integers and returna a single integer. Pattern matching is a way of deciding which definition of
a function to use depending on the input. Whether the input is a boolean, integer, floaat, they could all call a different definition of the function. If a pattern 
doesn't match the input Haskell may give an error.
Question: What were the first languages that put these to ideas into effect?

5. What is category theory? - Topos Institute
(https://www.youtube.com/watch?v=eXBwU9ieLL0)
Category theory is a branch of mathematics that focuses on relationships and structure, rather than on the internal details of any one area (like algebra or topology). 
In the video, the speaker contrasts this with group theory, which studies symmetry by looking at all the ways you can transform an object while preserving its structure, 
and then organizing those transformations into a system with axioms, subgroups, conjugacy classes, etc. The bigger point is that comparing different fields can
be hard because each one develops its own language and tools. Category theory is presented as a way to make those comparisons more natural by giving different subjects
a shared framework.
The speaker describes this framework using directed graphs made of nodes and arrows. In this view, the “things” you study in a field (for example, spaces in topology) 
can be treated as nodes, and the meaningful structure-preserving relationships between them become arrows. Different arrow styles can represent 
different kinds of relationships. Because it focuses on how objects connect and interact, category theory can be thought of as a level of abstraction above many standard 
theories, helping mathematicians translate ideas across subjects and build tools for tackling more complex theories later on.
Question: How is category theory useful outside of purely mathematics?

% =========================================================
\section{Conclusion}

  I found this class to test my critical thinking skills rather than my technical coding skills which was refreshing
after taking so many classes which are so heavily based on learning a specific language. I enjoyed the
focus on logic and reasoning, and I feel like I have a better understanding of how to approach problems in a
more abstract way. While I don't see this directly affecting my day to day coding in web development, I certainly
see this affecting my ability to think critically about problems and come up with solutions that are not obvious at first.
  I sincerely enjoyed the class and I feel like I learned a lot. I would recommend this class to anyone who is interested in
programming languages. I don't have much to recommend to improve the class, as I feel like the class was well structured
and the material built on itself while still covering a large range of topics.
  I specifically enjoyed digging into towers of hanoi. It was fun to see a easily undrstandable representation of a recursive
problem. I remember attempting this problem a couple of years ago and I found that this time it was eassier for me to complete.
I believe this is because of how my problem solving improved over the course of the class.
  I also thoroughly enjoyed the games on adam.math.hbu.de. I found it very useful to be able to try many different options 
and receiving quick feedback on whether or not it was the correct next move. I found that this helped me learn the material much more quickly
than if I had to work through every wrong answer I come up with on my way to the correct answer.
  My first aha moment in the class was the first assignment, the MU puzzle. I started by brute forcing the problem by trying
as many variations as I could but as I worked through it I realized that in the same way that i can prove something is possible
I can prove something is impossible. This shifted my mindset immediately to a point that I was much more open to finding solutions
that were not obvious at first.
  Overall, I am glad I took this class as I feel I am more prepared to think critically in all parts of my life after taking it, 
not just in software development. I noticed my problem solving skills improving week by week and I hope to feel that improvement in
other classes I take in the future as well.
% =========================================================
\section*{References}
https://adam.math.hhu.de/#/g/leanprover-community/nng4\\
https://adam.math.hhu.de/#/g/trequetrum/lean4game-logic \\
https://chatgpt.com/ \\
https://www.mathsisfun.com/games/towerofhanoi.html \\
https://www.overleaf.com/learn/latex/Learn_LaTeX_in_30_minutes \\ 



\end{document}
